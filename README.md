## 梯度下降法
[practice0112](https://github.com/morening/LearnML/blob/master/linear_regression/practice0112.py)

拟合 y = a*x1 + b*x2 + c

训练集 x:[[1, 2], [2, 1], [2, 3], [3, 5], [1, 3], [4, 2], [7, 3], [4, 5], [11, 3], [8, 7]]

训练集 y:[7, 8, 10, 14, 8, 13, 20, 16, 28, 26]

测试集 x:[[1, 4], [2, 2], [2, 5], [5, 3], [1, 5], [4, 1]]

**测试结果**
```
计算轮次：4009，耗时：0.434706，最小误差：0.000001
a = 2.000087
b = 1.000458
c = 2.997658
#测试：
[6.9986617712594938, 7.998290420402709, 9.9992069599056101, 14.00021041830318, 7.9991200410109435, 12.998922527943494, 19.999641554378947, 16.000297337197846, 27.999989229957613, 26.001561552279416]
#计算：
[8.999578310762395, 8.9987486901541605, 12.000123499408513, 15.99946771658961, 10.000036580513846, 11.998464258192042]
```

<img width="50%" height="50%" src="https://github.com/morening/LearnML/blob/master/snapshot/linear_regression/practice0112.png?raw=true" />

最后，感谢[《梯度下降原理及Python实现》](http://blog.csdn.net/programmer_wei/article/details/51941358)的帮助与指导，令我深刻理解梯度下降法的原理和完成python实现。

## 梯度下降法（矩阵）
[practice0115](https://github.com/morening/LearnML/blob/master/linear_regression/practice0115.py)

设 y = theta0 + theta1*x1 + theta2*x2

**测试结果**
```
#参数拟合：
[[ 2.9999716 ]
 [ 2.00000105]
 [ 1.00000556]]
#轮次：6528
#测试结果：
[[  6.99998377]
 [  7.99997927]
 [  9.99999038]
 [ 14.00000255]
 [  7.99998933]
 [ 12.99998693]
 [ 19.99999565]
 [ 16.00000361]
 [ 27.99999987]
 [ 26.00001894]]
```

